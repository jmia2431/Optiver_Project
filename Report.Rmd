---
title: "Investigation on the Significant Impact of Advanced Feature Engineering and Clustering on Model Performance"
output:
  pdf_document:
    keep_tex: true
header-includes:
  - \usepackage{multicol}
---
\newpage
\setlength{\columnsep}{20pt}
\tableofcontents
\newpage
\begin{multicols}{2}
\section[Abstract]{Abstract}
Financial markets are complex environments in which participants engage in the trading of various instruments within regulated marketplaces. This study examines the impact of feature engineering and clustering on model predictions in financial data analysis. The Weighted Average Price (WAP) is employed to calculate realised volatility, which serves as a key metric. By applying feature engineering techniques to bid and ask prices, spread measures are generated and incorporated into models such as Linear Regression (LR), Light Gradient Boosting Machine (LGBM), Random Forest (RF), and time series models such as Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) and AutoRegressive Integrated Moving Average (ARIMA). The efficacy of these models is evaluated using the Symmetric Mean Absolute Percentage Error (SMAPE) metric. Moreover, an interactive Shiny application is developed to visualise predictive volatility using an ensemble model. In a focused analysis of Optiver's stock prices, the ensemble model demonstrated superior performance compared to other models, indicating the effectiveness of combining machine learning models with time series forecasting techniques.
\section[Method]{Method}
\subsection[Volatility Calculation]{Volatility Calculation}
Realised volatility $\sigma$ is defined as the square root of the sum of consecutive log returns $d$, calculated using the Weighted Average Price (WAP).
$$WAP = \frac{\text{BidPrice}_1\cdot\text{AskSize}_1+\text{AskPrice}_1\cdot\text{BidSize}_1} {\text{BidSize}_1+\text{AskSize}_1}$$ 
$$d = \log\left( \frac{\text{WAP}_{t-1}}{\text{WAP}_1} \right)$$
$$\sigma = \sqrt{\sum_{i=1}^{n} d_i^2}$$

The significance of models is determined by the Symmetric Mean Absolute Percentage Error (SMAPE) metric, which enables the comparison of errors across different scales and the filtering of extreme values to prevent an infinite error rate.
$$\text{SMAPE} = 100 \times \text{mean}\left( \frac{\left| \text{prediction} - \text{test} \right|}{\frac{\left( \left| \text{test} \right| + \left| \text{prediction} \right| \right)}{2}} \right)$$
\subsection[Machine Learning Model]{Machine Learning Model}
\subsubsection[LightGBM]{LightGBM}
We chose the LightGBM (Light Gradient Boosting Machine) algorithm given the high-dimensional nature of our dataset, which includes detailed trading data for various stocks such as bid and ask prices, along with other transaction details.The LightGBM model is configured for predicting continuous values with carefully selected hyper-parameters to enhance performance and avoid overfitting. The model operates with a regression objective and a conservative learning rate of 0.01, ensuring slow, precise learning. Feature and data sampling are both set at 80%, using `colsample_bytree` and `subsample` respectively, to promote model robustness by reducing variance and preventing memorization. Trees are restricted to a depth of five to balance complexity with generalization. The `verbose` setting is minimized to streamline training, making it ideal for automated setups or integrated applications, focusing on efficient and effective stock volatility prediction.

During the model training phase we incorporated K-Fold Cross-Validation to ensure the model's robustness and prevent overfitting. This involved dividing the data into five segments, using each segment in turn as a test set while training the model on the remaining four. Additionally, we utilized weighted training, applying varying importance to different segments of the data, particularly where there were significant fluctuations in stock prices, to focus the model on these critical areas.

Overall, the model demonstrated high accuracy and excellent performance compared to simpler models across various stocks, confirming the effectiveness of our approach in predicting stock market volatility.
\subsection[Time Series Model]{Time Series Model}
\subsubsection[ARIMA]{ARIMA}
In this study, an ARIMA model was employed to predict stock volatility. The ARIMA model is a widely-used approach for forecasting time series data, particularly in the field of finance[1]. In this instance, an ARIMA(3,1,1) model was used. This parameter, p = 3, incorporates the model's use of the past three observations to predict the current value. The parameter d = 1 signifies that the data was differenced once to achieve stationarity, thereby ensuring that the time series is stable over time by removing trends. The parameter q = 1 denotes that the model includes one moving average term, which utilises the past forecast error to enhance the accuracy of the prediction. The optimal values for p, d and q were selected through a combination of theoretical knowledge and empirical analysis, involving the testing of multiple parameter sets. The ARIMA(3,1,1) model exhibited a low SMAPE value, indicative of high accuracy, and a low distribution of SMAPE, demonstrating consistency in the predictions.

\subsubsection[Moving Window]{Moving Window}

The implementation of a moving window approach in conjunction with the ARIMA model enables the model to adapt dynamically to evolving patterns within the dataset, thereby enhancing its predictive capabilities[2]. By employing a sliding window of a specified size, the model is able to respond in real-time to shifting data dynamics, thereby facilitating accurate and timely predictions. In this study, the 'step' parameter was set to 4, corresponding to a 60-second prediction window. The original dataset consisting of 40 volatilities was divided into 36 for training and 4 for testing. This allocation of data ensured a balanced evaluation of the model's predictive performance, allowing for a comprehensive assessment of its ability to accurately forecast future volatility.

\end{multicols}
