{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1.5px solid gray\">\n",
    "\n",
    "# DATA3888 Final Report\n",
    "\n",
    "<hr style=\"border:1.5px solid gray\">\n",
    "\n",
    "## Optiver 1\n",
    "\n",
    "---\n",
    "SIDs: 520434835, \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "Financial markets are complicated systems where participants buy or sell financial instruments, such as stocks, bonds, derivatives, currencies and commodities, through a structured marketplace under regulatory oversight. Financial data provides a rich environment for data analysis through the investigation of market research, investment strategies, risk management, algorithmic trading, and predictive modelling. An order book provides one instance of a relatively simplistic dataset conveying the bid-ask prices and sizes of particular stocks at a given moment in the market. High-Frequency Trading (HFT) market makers like Optiver provide liquidity in the market to make small amounts of profit over large volumes of trades. For these trading firms, capturing price fluctuations of the products being traded is invaluable for their profits; through this report, we discuss the impact of feature engineering and clustering on the performance of model predictions.\n",
    "\n",
    "Realised volatility is defined as the square root of the sum of consecutive log returns, calculated using the Weighted Average Price (WAP).\n",
    "$$\n",
    "WAP = \\frac{\\text{BidPrice}_1\\cdot\\text{AskSize}_1+\\text{AskPrice}_1\\cdot\\text{BidSize}_1} {\\text{BidSize}_1+\\text{AskSize}_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop more variables for our models in addition to the attributes in the stock datasets, we applied feature engineering on the bid and ask prices and sizes, creating various measures of spreads for the time ids. Through this, we compare our optimised linear regression model with a baseline linear model. The feature engineering variables were also used in our tree based models, Light GBM and Random Forest. Comparatively, time series models like GARCH and AutoRegressive Integrated Moving Average(ARIMA) were also developed. These various models were developed and compared to determine the prediction of volatility with the highest accuracy whilst reducing the error rate.\n",
    " \n",
    "*Write something on clustering?*\n",
    "\n",
<<<<<<< HEAD
    "Our final product for traders to utilise is an interactive shiny application that allows users to upload stock data and visualise the predictive volatility through an optimised ensemble model that combines all models’ predictions and chooses the lowest accuracy.\n"
=======
    "Our final product for traders to utilise is an interactive shiny application that allows users to upload stock data and visualise the predictive volatility through an optimised ensemble model that combines all models’ predictions and chooses the lowest accuracy.\n",
    "\n",
    "log return <br/>\n",
    "volatility <br/>\n",
    "Symmetric Mean Absolute Percentage Error \n"
>>>>>>> 2937dcfe1efdeb007d8096df860df8a76a4094c4
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To capture the subtle relationships between our features and the target\n",
    "variable (one-minute volatility), we calculated around 150 features, all\n",
    "interpretable as measurements of liquidity or volatility from the previous 9\n",
    "minutes data.\n",
    "\n",
    "Initially, for each second, we calculated features to measure differences\n",
    "between bid-ask prices, bid prices, and ask prices. We also computed price\n",
    "features including WAP1 and WAP2, and used them with bid/ask prices to calculate\n",
    "log returns.\n",
    "\n",
    "After calculating these second-level features, we aggregated them using\n",
    "different functions to capture various aspects of the data. For instance, for\n",
    "log-return features of different prices, we used sum and realized volatility.\n",
    "For price features, we used mean and standard deviation, etc. Additionally, we\n",
    "applied four different aggregation ranges to capture time dependencies between\n",
    "features and our target.\n",
    "\n",
    "Furthermore, we used four different aggregation ranges to capture the time\n",
    "dependence between features and our target. For instance, we created subsets of\n",
    "specific time ranges to perform the aggregation. We started by aggregating\n",
    "features over the entire past 9 minutes (540 seconds), then used subsets of the\n",
    "past 6 minutes before the 9th minute, the past 3 minutes before the 9th minute,\n",
    "and so on.\n",
    "\n",
    "After completing the entire process, we generated 150 features for every\n",
    "time-id in every stock-id, calculated based on the previous 540 seconds of data.\n",
    "Our target variable was the volatility for the period between 540 and 600 seconds.\n",
    "This thorough feature engineering process allowed us to capture essential\n",
    "aspects of liquidity and volatility over different time frames, significantly\n",
    "enhancing the predictive power of our model.\n",
    "\n",
    "The whole process are performed using multiprocessing technique, which can\n",
    "be solved by only 4 minutes for all stocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feeewf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM \n",
    "\n",
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
    "### GARCH\n",
    "\n",
    "The Generalized AutoRegressive Conditional Heteroskedasticity Model, known as the GARCH model is a statistical model used in analyzing time-series data where the variance error is believed to be serially autocorrelated [1]. The GARCH model we developed is the most well-known one as GARCH(1,1). The parameters in GARCH are named p and q, which represent the variance term, and squared residual term, respectively.\n",
    "In Python, this model is imported from ‘arch_model’ in the arch module. Before the training, each log return is multiplied by 10^4 for better fitting. To meet our goal of predicting volatility in the future in one minute, the GARCH model is called by simulating 60 steps (as 60 seconds) 1000 times to ensure the results are not too variable. As a result, we have got a matrix of 60*1000 simulated log returns. The model-building process and the results are represented as follows.\n",
    "![img](./Pictures/GARCH_Flow.png)\n",
    "Then, we calculate the volatility of each path and take the median value as the final prediction. The reason why we median is used because it is the best performanced result. The volatility predicted by the GARCH model can be calculated in many ways since the model only gives simulated log returns as a result. We developed four different ways to generate volatilities in this figure. One is from the axis of each step, and the other three are from the axis of each path in the simulation result matrix. The median of volatilities calculated from each path has the best performance evaluated by SMAPE. Therefore, we use the median calculation method in our final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, an ARIMA model was employed to predict stock volatility. The ARIMA model is a widely-used approach for forecasting time series data, particularly in the field of finance[1]. In this instance, an ARIMA(3,1,1) model was used. This parameter, p = 3, incorporates the model's use of the past three observations to predict the current value. The parameter d = 1 signifies that the data was differenced once to achieve stationarity, thereby ensuring that the time series is stable over time by removing trends. The parameter q = 1 denotes that the model includes one moving average term, which utilises the past forecast error to enhance the accuracy of the prediction. The optimal values for p, d and q were selected through a combination of theoretical knowledge and empirical analysis, involving the testing of multiple parameter sets. The ARIMA(3,1,1) model exhibited a low root mean square error (RMSE) value, indicative of high accuracy, and a low distribution of RMSE, demonstrating consistency in the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of a moving window approach in conjunction with the ARIMA model enables the model to adapt dynamically to evolving patterns within the dataset, thereby enhancing its predictive capabilities[2]. By employing a sliding window of a specified size, the model is able to respond in real-time to shifting data dynamics, thereby facilitating accurate and timely predictions. In this study, the 'step' parameter was set to 4, corresponding to a 60-second prediction window. The original dataset consisting of 40 volatilities was divided into 36 for training and 4 for testing. This allocation of data ensured a balanced evaluation of the model's predictive performance, allowing for a comprehensive assessment of its ability to accurately forecast future volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "Ensemble learning, which combines multiple models, enhances the robustness and accuracy of predictions by leveraging the strengths of various underlying algorithms. We utilized the Symmetric Mean Absolute Percentage Error (SMAPE) as our evaluation metric to provide a balanced view of prediction errors.\n",
    "\n",
    "In our study, we evaluated the performance of several predictive models including a Baseline Linear Regression, Linear Regression, GARCH (1,1), Random Forest, ARIMA (3,1,1), LightGBM, and an Ensemble that integrates outputs from all these models.\n",
    "\n",
    "Our analysis revealed that the Ensemble model, with the lowest SMAPE value of 28.8485, significantly outperformed the individual models in terms of accuracy. The Random Forest and LGB models also displayed strong performances with SMAPE scores closely trailing the Ensemble model, though their processing times varied considerably. On the other hand, the GARCH (1,1) and ARIMA (3,1,1) models, typically favored for time series forecasting, were outperformed by tree-based models in this instance. Notably, the processing time for the ARIMA model was higher, which could limit its practicality in scenarios that require rapid model retraining.\n",
    "\n",
    "Overall, the use of ensemble learning in forecasting has shown to enhance performance markedly over individual models. This approach effectively captures the unique strengths of each model, leading to more accurate and robust predictions. The Ensemble model not only delivered improved accuracy but also maintained reasonable prediction times, making it well-suited for real-time forecasting applications. Looking ahead, further optimization of the ensemble components and their weights might yield even better results. Additionally, future efforts should explore the balance between processing time and accuracy to customize the ensemble approach for specific operational requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 2937dcfe1efdeb007d8096df860df8a76a4094c4
    "### Methods of Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMAPE is used as evaluation metric because it express the error in terms of\n",
    "percentage, make it easy to interpret. Also, its denominator can filter extreme\n",
    "values, prevent the issue of infinity which is common among MAPE.\n",
    "Run through a loop of all stocks, we perform a 5-fold cross validation, for\n",
    "each iteration, we train the model with 80 percent shuffled data, and use the\n",
    "rest 20 percent data to make prediction, we record the prediction for these out\n",
    "of fold data. Therefore, after the whole process, we have out of fold prediction\n",
    "for every row of this stock’s data. We extend the process to all of our models\n",
    "and all of our stocks, and merge the result, we finally collected a dataset\n",
    "called performance table, record SMAPE for every time-id and every stock-id, and\n",
    "the column name is different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shiny\n",
    "\n",
    "The deployment process of the machine learning model, aimed at predicting stock volatility, was carefully executed to ensure seamless integration and usability within the Python based Shiny app (https://nithya7612.shinyapps.io/my-app/). The app provides traders with quick and accurate volatility forecasts for a range of stocks, enhancing their decision-making processes in dynamic financial markets.\n",
    "\n",
    "The app mimics real-time streaming of data and provides access to the Optiver stock database, delivering an authentic trading experience. Currently, it includes 20 stock files, allowing users to select from these pre-loaded options. Due to Posit subscription plans, all 100 stock files could not be deployed on the free plan, so only 1GB worth of data was uploaded. Once a file is chosen, the app swiftly performs volatility predictions within approximately one second, ensuring minimal delay. Traders simply need to select the stock ID to receive one-minute ahead predictions, making it a highly efficient tool for rapid decision-making. This seamless process ensures that traders have timely and precise volatility data at their fingertips, enhancing their ability to react swiftly to market changes. Shiny for Python was used to design the prediction app. In order to deploy the app, the code:\n",
    "`rsconnect deploy shiny ./dashboard/ --name nithya7612 –title my-app` was executed in the terminal. Rsconnect first verifies the connection to the Shiny server to ensure it reachable and verifies the app mode is compatible with the Shiny deployment requirements. When deploying a Shiny for Python application, the next step involves making a bundle. The deployment tool gathers all the necessary files from the application directory, including python scripts, data files and CSS scripts. The requirements.txt is file is parsed to identify all the dependencies required by the application. All this data is compressed into a single bundle file, to streamline the upload process by reducing the number of individual file transfers and includes all the metadata about the file.\n",
    "\n",
    "The next stage is to deploy the bundle itself. The bundle is uploaded to the server and then unpacked. This involves decompressing the archive and extracting all the files appropriately. The server creates an environment for the application to run in, by installing all the files (python scripts etc.) and the required dependencies (requirements.txt). Images of the application are then pushed to the server. Following this, the server begins the staging phase preparing for deployment. A new instance of the\n",
    "application if rolled forward and any old instances are terminated. The final stage confirms that the application runs as expected on the server by “Verifying Deploying Content”. A multidisciplinary approach was crucial to the development of the volatility prediction application, integrating finance, data science and human computer interactions. Finance underpins the app’s\n",
    "purpose, ensuring it meets the financial needs and objectives of Optiver Option’s Traders. Data Science provides the models that were incorporated for the prediction and accuracies of the volatility prediction model. Human-Computer Interaction focuses on the optimising how options traders interact with the application ensuring a user-friendly interface.\n",
    "\n",
    "### Future Work\n",
    "Future work for the app involves continuously updating and improving the model and its predictive capabilities by training on data that is received throughout the day. Each night, once trading closes, the app will use latest stock data to create a new model ensuring the predictions are based on the most recent information. This approach guarantees that each day, the app operates with a freshly trained model, enhancing accuracy and relevance in the dynamic stock market environment. By prioritising more recent data, the model better captures the latest market trends and shifts, which have a greater impact on future volatility compared to significantly older data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]https://www.sciencedirect.com/science/article/pii/S0020025523015360?casa_token=1rjJEPgVSvsAAAAA:iITm0ttv6qfGLVIvuPKeL-xbOJ7GZrDO03L140nkd2CN5PXTX1WxsZCjc3xCsGqhmJ5kQN3VPQ#br0240 <br/>\n",
    "[2]​​https://arxiv.org/abs/2405.08284"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Contribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
