{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1.5px solid gray\">\n",
    "\n",
    "# DATA3888 Final Report\n",
    "\n",
    "<hr style=\"border:1.5px solid gray\">\n",
    "\n",
    "## Optiver 1\n",
    "\n",
    "---\n",
    "SIDs: 520434835, \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "Financial markets are complicated systems where participants buy or sell financial instruments, such as stocks, bonds, derivatives, currencies and commodities, through a structured marketplace under regulatory oversight. Financial data provides a rich environment for data analysis through the investigation of market research, investment strategies, risk management, algorithmic trading, and predictive modelling. An order book provides one instance of a relatively simplistic dataset conveying the bid-ask prices and sizes of particular stocks at a given moment in the market. High-Frequency Trading (HFT) market makers like Optiver provide liquidity in the market to make small amounts of profit over large volumes of trades. For these trading firms, capturing price fluctuations of the products being traded is invaluable for their profits; through this report, we discuss the impact of feature engineering and clustering on the performance of model predictions.\n",
    "\n",
    "Realised volatility is defined as the square root of the sum of consecutive log returns, calculated using the Weighted Average Price (WAP).\n",
    "$$\n",
    "WAP = \\frac{\\text{BidPrice}_1\\cdot\\text{AskSize}_1+\\text{AskPrice}_1\\cdot\\text{BidSize}_1} {\\text{BidSize}_1+\\text{AskSize}_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop more variables for our models in addition to the attributes in the stock datasets, we applied feature engineering on the bid and ask prices and sizes, creating various measures of spreads for the time ids. Through this, we compare our optimised linear regression model with a baseline linear model. The feature engineering variables were also used in our tree based models, Light GBM and Random Forest. Comparatively, time series models like GARCH and ARIMA were also developed. These various models were developed and compared to determine the prediction of volatility with the highest accuracy whilst reducing the error rate.\n",
    " \n",
    "*Write something on clustering?*\n",
    "\n",
    "Our final product for traders to utilise is an interactive shiny application that allows users to upload stock data and visualise the predictive volatility through an optimised ensemble model that combines all models’ predictions and chooses the lowest accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figure 1.drawio.png\" alt=\"figure 1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "### LightGBM \n",
    "\n",
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GARCH\n",
    "\n",
    "The Generalized AutoRegressive Conditional Heteroskedasticity Model, known as the GARCH model is a statistical model used in analyzing time-series data where the variance error is believed to be serially autocorrelated [1]. The GARCH model we developed is the most well-known one as GARCH(1,1). The parameters in GARCH are named p and q, which represent the variance term, and squared residual term, respectively.\n",
    "In Python, this model is imported from ‘arch_model’ in the arch module. Before the training, each log return is multiplied by 10^4 for better fitting. To meet our goal of predicting volatility in the future in one minute, the GARCH model is called by simulating 60 steps (as 60 seconds) 1000 times to ensure the results are not too variable. As a result, we have got a matrix of 60*1000 simulated log returns. The model-building process and the results are represented as follows.\n",
    "![img](./Pictures/GARCH_Flow.png)\n",
    "Then, we calculate the volatility of each path and take the median value as the final prediction. The reason why we median is used because it is the best performanced result. The volatility predicted by the GARCH model can be calculated in many ways since the model only gives simulated log returns as a result. We developed four different ways to generate volatilities in this figure. One is from the axis of each step, and the other three are from the axis of each path in the simulation result matrix. The median of volatilities calculated from each path has the best performance evaluated by SMAPE. Therefore, we use the median calculation method in our final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Shiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
